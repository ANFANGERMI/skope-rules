"""
This is a module to be used as a reference for building other modules
"""
import numpy as np
import pandas  # XXX to be rm
import numbers
from warnings import warn
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor

from sklearn.ensemble import BaggingClassifier, BaggingRegressor

from sklearn.externals import six
from sklearn.tree import _tree

INTEGER_TYPES = (numbers.Integral, np.integer)


class FraudToRules(BaseEstimator, ClassifierMixin):
    """ An easy-interpretable classifier optimizing simple logical rules.

    Parameters
    ----------

    feature_names: list of str, optional (default=None)
        XXX (remove it if we want generic tool)
        The names of each feature to be used for returning rules in string
        format.

    BAGGING PARAMETERS:

    n_estimators : int, optional (default=1)
        The number of base estimators (rules) to build.

    max_samples : int or float, optional (default=1.)
        The number of samples to draw from X to train each decision tree, from
        which rules are generated and selected.
            - If int, then draw `max_samples` samples.
            - If float, then draw `max_samples * X.shape[0]` samples.
        If max_samples is larger than the number of samples provided,
        all samples will be used for all trees (no sampling).

    max_samples_features : int or float, optional (default=1.0)
        The number of features to draw from X to train each decision tree.
            - If int, then draw `max_features` features.
            - If float, then draw `max_features * X.shape[1]` features.

    bootstrap : boolean, optional (default=True)
        Whether samples are drawn with replacement.

    bootstrap_features : boolean, optional (default=False)
        Whether features are drawn with replacement.


    BASE ESTIMATORS PARAMETERS:

    max_depth : integer or None, optional (default=None)
        The maximum depth of the decision trees. If None, then nodes are
        expanded until all leaves are pure or until all leaves contain less
        than min_samples_split samples.  XXX faisable en pratique?

    max_features : int, float, string or None, optional (default="auto")
        The number of features considered (by each decision tree) when looking
        for the best split:

        - If int, then consider `max_features` features at each split.
        - If float, then `max_features` is a percentage and
          `int(max_features * n_features)` features are considered at each
          split.
        - If "auto", then `max_features=sqrt(n_features)`.
        - If "sqrt", then `max_features=sqrt(n_features)` (same as "auto").
        - If "log2", then `max_features=log2(n_features)`.
        - If None, then `max_features=n_features`.

        Note: the search for a split does not stop until at least one
        valid partition of the node samples is found, even if it requires to
        effectively inspect more than ``max_features`` features.

    min_samples_split : int, float, optional (default=2)
        The minimum number of samples required to split an internal node for
        each decision tree.
        - If int, then consider `min_samples_split` as the minimum number.
        - If float, then `min_samples_split` is a percentage and
          `ceil(min_samples_split * n_samples)` are the minimum
          number of samples for each split.

    XXX should we add more DecisionTree params?

    GENERAL PARAMETERS:

    n_jobs : integer, optional (default=1)
        The number of jobs to run in parallel for both `fit` and `predict`.
        If -1, then the number of jobs is set to the number of cores.

    random_state : int, RandomState instance or None, optional (default=None)
        If int, random_state is the seed used by the random number generator;
        If RandomState instance, random_state is the random number generator;
        If None, the random number generator is the RandomState instance used
        by `np.random`.

    verbose : int, optional (default=0)
        Controls the verbosity of the tree building process.

    Attributes
    ----------
    rules_ : list of selected rules.
        The collection of rules generated by fitted sub-estimators (decision
        trees) and further selected according to their respective precisions.


    estimators_ : list of DecisionTreeClassifier
        The collection of fitted sub-estimators used to generate candidate
        rules.

    estimators_samples_ : list of arrays
        The subset of drawn samples (i.e., the in-bag samples) for each base
        estimator.

    estimators_features_ : list of arrays
        The subset of drawn features for each base estimator.

    max_samples_ : integer
        The actual number of samples
    """

    def __init__(self,
                 n_estimators=1,
                 feature_names=None,
                 max_samples=1.,
                 max_samples_features=1.,
                 max_depth=5,
                 max_features=1.,
                 min_samples_split=2,
                 bootstrap=False,
                 bootstrap_features=False,
                 n_jobs=1,
                 random_state=None,
                 verbose=0):
        self.n_estimators = n_estimators
        self.feature_names = feature_names
        self.max_samples = max_samples
        self.max_samples_features = max_samples_features
        self.max_depth = max_depth
        self.max_features = max_features
        self.min_samples_split = min_samples_split
        self.bootstrap = bootstrap
        self.bootstrap_features = bootstrap_features
        self.n_jobs = n_jobs
        self.random_state = random_state
        self.verbose = verbose

    def fit(self, X, y, sample_weight=None):
        """Fit the model according to the given training data.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Training vector, where n_samples is the number of samples and
            n_features is the number of features. XXX sparse matrix?

        y : array-like, shape (n_samples,)
            Target vector relative to X. Following convention bigger is better,
            frauds have to be labeled as -1, and normal data as 1.

        sample_weight : array-like, shape (n_samples,) optional
            Array of weights that are assigned to individual samples, typically
            the amount in case of transactions data. Used to grow regression
            trees producing further rules to be tested.
            If not provided, then each sample is given unit weight.

        Returns
        -------
        self : object
            Returns self.
        """

        X, y = check_X_y(X, y)

        # ensure that max_samples is in [1, n_samples]:
        n_samples = X.shape[0]

        if isinstance(self.max_samples, six.string_types):
            raise ValueError('max_samples (%s) is not supported.'
                             'Valid choices are: "auto", int or'
                             'float' % self.max_samples)

        elif isinstance(self.max_samples, INTEGER_TYPES):
            if self.max_samples > n_samples:
                warn("max_samples (%s) is greater than the "
                     "total number of samples (%s). max_samples "
                     "will be set to n_samples for estimation."
                     % (self.max_samples, n_samples))
                max_samples = n_samples
            else:
                max_samples = self.max_samples
        else:  # float
            if not (0. < self.max_samples <= 1.):
                raise ValueError("max_samples must be in (0, 1], got %r"
                                 % self.max_samples)
            max_samples = int(self.max_samples * X.shape[0])

        self.max_samples_ = max_samples

        self.rules_ = []
        self.estimators_ = []
        self.estimators_samples_ = []
        self.estimators_features_ = []

        # default columns names of the form ['c0', 'c1', ...]:
        feature_names_ = (self.feature_names if self.feature_names is not None
                          else ['c' + x for x in
                                np.arange(X.shape[1]).astype(str)])
        self.feature_names_ = feature_names_

        bagging_clf = BaggingClassifier(
            base_estimator=DecisionTreeClassifier(),
            n_estimators=self.n_estimators,
            max_samples=self.max_samples_,
            max_features=self.max_samples_features,
            bootstrap=self.bootstrap,
            bootstrap_features=self.bootstrap_features,
            # oob_score=... XXX may be added if selection on tree perf needed.
            # warm_start=... XXX may be added to increase computation perf.
            n_jobs=self.n_jobs,
            random_state=self.random_state,
            verbose=self.verbose)

        bagging_reg = BaggingRegressor(
            base_estimator=DecisionTreeRegressor(),
            n_estimators=self.n_estimators,
            max_samples=self.max_samples_,
            max_features=self.max_samples_features,
            bootstrap=self.bootstrap,
            bootstrap_features=self.bootstrap_features,
            # oob_score=... XXX may be added if selection on tree perf needed.
            # warm_start=... XXX may be added to increase computation perf.
            n_jobs=self.n_jobs,
            random_state=self.random_state,
            verbose=self.verbose)

        #import pdb; pdb.set_trace()
        bagging_clf.fit(X, y)
        y_reg = y  # XXX todo define y_reg
        bagging_reg.fit(X, y_reg)

        self.estimators_ += bagging_clf.estimators_
        self.estimators_ += bagging_reg.estimators_

        self.estimators_samples_ += bagging_clf.estimators_samples_
        self.estimators_samples_ += bagging_reg.estimators_samples_

        self.estimators_features_ += bagging_clf.estimators_features_
        self.estimators_features_ += bagging_reg.estimators_features_

        for estimator, samples, features in zip(self.estimators_,
                                                self.estimators_samples_,
                                                self.estimators_features_):

            # Create mask for OOB samples
            mask = ~samples
            rules_from_tree = self._tree_to_rules(estimator, self.feature_names_)

            # XXX todo: idem without dataframe
            X_oob = pandas.DataFrame((X[mask, :])[:, features],
                                     columns=self.feature_names_)
            y_oob = y[mask]

            # Add OOB performances to rules:
            rules_from_tree = [(r, (1 - y_oob[
                list(X_oob.query(r).index)].mean()) / 2)
                               for r in rules_from_tree]
            self.rules_ += rules_from_tree

        # for _ in range(self.n_estimators):
        #     # XXX TODO: use max_samples and bootstrap params
        #     clf = DecisionTreeClassifier(
        #         max_features=self.max_features,
        #         max_depth=self.max_depth,
        #         min_samples_split=self.min_samples_split)
        #     clf.fit(X_train, y_train)
        #     self.estimators_.append(clf)
        #     rules = self._tree_to_rules(clf, feature_names_)
        #     self.rules_ += rules

        # for _ in range(self.n_estimators):
        #     clf = DecisionTreeRegressor(
        #         max_features=self.max_features,
        #         max_depth=self.max_depth,
        #         min_samples_split=self.min_samples_split)
        #     clf.fit(X_train, y_train)
        #     self.estimators_.append(clf)
        #     rules = self._tree_to_rules(clf, feature_names_)
        #     self.rules_ += rules

        self.rules_ = sorted(self.rules_, key=lambda x: -x[1])

        return self

    def predict(self, X):
        """Predict if a particular sample is an outlier or not.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            The input samples. Internally, it will be converted to
            ``dtype=np.float32`` XXX allow sparse matrix?

        Returns
        -------
        is_inlier : array, shape (n_samples,)
            For each observations, tells whether or not (+1 or -1) it should
            be considered as an inlier according to the fitted model.
        """

        # Check if fit had been called
        check_is_fitted(self, ['rules_', 'estimators_', 'estimators_samples_',
                               'max_samples_'])

        # Input validation
        X = check_array(X)

        return 2 * (self.decision_function(X) == 0) - 1

    def decision_function(self, X):
        """Average anomaly score of X of the base classifiers (rules).

        The anomaly score of an input sample is computed as
        the negative weighted sum of the binary rules outputs. The weight is
        the respective precision of each rule.

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            The training input samples.

        Returns
        -------
        scores : array, shape (n_samples,)
            The anomaly score of the input samples.
            The lower, the more abnormal. Negative scores represent outliers,
            null scores represent inliers.

        """
        selected_rules = self.rules_[:self.n_estimators]
        df = pandas.DataFrame(X, columns=self.feature_names_)

        scores = np.zeros(X.shape[0])
        for (r, w) in selected_rules:
            scores[list(df.query(r).index)] += w

        scores = -scores  # "bigger is better" convention (here less abnormal)
        return scores

    def _tree_to_rules(self, tree, feature_names):
        """
        Return a list of rules from a tree

        Parameters
        ----------
            tree : Decision Tree Classifier/Regressor
            feature_names: list of variable names

        Returns
        -------
        rules : list of rules.
        """
        # XXX todo: check the case where tree is build on subset of features,
        # ie max_features != None

        tree_ = tree.tree_
        feature_name = [
            feature_names[i] if i != _tree.TREE_UNDEFINED else "undefined!"
            for i in tree_.feature
        ]
        rules = []

        def recurse(node, base_name):
            if tree_.feature[node] != _tree.TREE_UNDEFINED:
                name = feature_name[node]
                symbol = '<='
                symbol2 = '>'
                threshold = tree_.threshold[node]
                text = base_name + ["{} {} {}".format(name, symbol, threshold)]
                recurse(tree_.children_left[node], text)

                text = base_name + ["{} {} {}".format(name, symbol2,
                                                      threshold)]
                recurse(tree_.children_right[node], text)
            else:
                rules.append(str.join(' and ', base_name))

        recurse(0, [])

        return rules




if __name__ == '__main__':
    rnd = np.random.RandomState(0)
    X = 3 * rnd.uniform(size=(50, 5)).astype(np.float32)
    y = np.array([1] * (X.shape[0] - 10) + [0] * 10)
    clf = FraudToRules()
    clf.fit(X, y)
    import pdb; pdb.set_trace()
    clf.predict(X)
